{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "num_epoch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dset.MNIST(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "mnist_test = dset.MNIST(\"./\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706, 0.4941, 0.5333,\n",
      "         0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1176,\n",
      "         0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "         0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922, 0.9333,\n",
      "         0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9843,\n",
      "         0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.8588,\n",
      "         0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137, 0.9686, 0.9451,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3137,\n",
      "         0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000, 0.1686, 0.6039,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275, 0.4235, 0.0039,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922, 0.9922, 0.4667,\n",
      "         0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294, 0.9922, 0.9922,\n",
      "         0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.3647, 0.9882,\n",
      "         0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9765,\n",
      "         0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098, 0.7176, 0.9922,\n",
      "         0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922, 0.9922, 0.9922,\n",
      "         0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922, 0.9922, 0.7882,\n",
      "         0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0902,\n",
      "         0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.3176, 0.0078,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706, 0.8588,\n",
      "         0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922, 0.9922,\n",
      "         0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922, 0.8314,\n",
      "         0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff63c1e11c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(mnist_train[0][0].view(28,-1))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(torch.squeeze(mnist_train[0][0].view(28,-1)),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(mnist_train,batch_size=batch_size, shuffle=True,num_workers=2,drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test,batch_size=batch_size, shuffle=False,num_workers=2,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.Conv2d(1,16,3,padding=1),                            # batch x 16 x 28 x 28\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(16),\n",
    "                        nn.Conv2d(16,32,3,padding=1),                           # batch x 32 x 28 x 28\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(32),\n",
    "                        nn.Conv2d(32,64,3,padding=1),                           # batch x 32 x 28 x 28\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.MaxPool2d(2,2)                                       # batch x 64 x 14 x 14\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.Conv2d(64,128,3,padding=1),                          # batch x 64 x 14 x 14\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.MaxPool2d(2,2),\n",
    "                        nn.Conv2d(128,256,3,padding=1),                         # batch x 64 x 7 x 7\n",
    "                        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "                \n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(batch_size, -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(256,128,3,2,1,1),                    # batch x 128 x 14 x 14\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.ConvTranspose2d(128,64,3,1,1),                       # batch x 64 x 14 x 14\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(64,16,3,1,1),                        # batch x 16 x 14 x 14\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(16),\n",
    "                        nn.ConvTranspose2d(16,1,3,2,1,1),                       # batch x 1 x 28 x 28\n",
    "                        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x.view(batch_size,256,7,7)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roy/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "\n",
    "# 인코더 디코더의 파라미터를 동시에 학습시키기 위해 이를 묶는 방법입니다.\n",
    "parameters = list(encoder.parameters())+ list(decoder.parameters())\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-eca10dce8e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(num_epoch):\n",
    "    for j,[image,label] in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        image = image.to(device)\n",
    "        \n",
    "        output = encoder(image)\n",
    "        output = decoder(output)\n",
    "        \n",
    "        loss = loss_func(output,image)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if j % 10 == 0:\n",
    "        # 모델 저장하는 방법\n",
    "        # 이 역시 크게 두가지 방법이 있는데 여기 사용된 방법은 좀 단순한 방법입니다.\n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        torch.save([encoder,decoder],'./model/conv_autoencoder.pkl')\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img = torch.squeeze(output.cpu().data)\n",
    "print(out_img.size())\n",
    "\n",
    "for i in range(2):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(torch.squeeze(image[i]).cpu().numpy(),cmap='gray')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(out_img[i].numpy(),cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for j,[image,label] in enumerate(test_loader):\n",
    "       \n",
    "        image = image.to(device)\n",
    "        output = encoder(image)\n",
    "        output = decoder(output)\n",
    "        \n",
    "    if j % 10 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_img = torch.squeeze(output.cpu().data)\n",
    "print(out_img.size())\n",
    "\n",
    "for i in range(2):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(torch.squeeze(image[i]).cpu().numpy(),cmap='gray')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(out_img[i].numpy(),cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
